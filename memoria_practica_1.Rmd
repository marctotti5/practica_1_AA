---
title: ""
author: ""
date: ""
geometry: "left=3cm,right=3cm,top=3cm,bottom=3cm"
header_includes:
 - \usepackage{longtable}
 - \usepackage{lscape}
output: 
        pdf_document:
                includes:
                        in_header: "wrap-code.tex"
                        before_body: "portada.sty"
                toc: true
                toc_depth: 6
                number_sections: true
institute: "Universidad Carlos III de Madrid"
documentclass: "article"
papersize: a4
linestrech: 1.5
fontsize: 11pt
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
\section{Objetivo}
Crear modelos de regresión para predecir la radiación solar, a partir de predicciones de variables meteorológicas.

\section{Problema}
A finales del 2015 se estimó que el 23,7% de la energía eléctrica mundial se produjo mediante fuentes de energía renovables.

Uno de los mayores problemas que tiene la energía solar es su variabilidad e incertidumbre. Las empresas productoras necesitan una estimación diaria para las siguientes 24 horas. Por ello es importante tener una predicción lo más acertada posible.

Nuestro principal objetivo será predecir la radiación solar diaria en una planta solar de Oklahoma a partir de predicciones de variables
meteorológicas del día anterior, usando MLR.

\section{Datos}
Tenemos el archivo número 21. Vamos a leer los datos `disp21.rds`, que contienen todos los datos de entrenamiento y validación y luego el conjunto `comp21.rds`, que contienen todos los datos para realizar las predicciones de competición.
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(mlr3)
library(mlr3verse)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3filters)
library(paradox)
library(mlr3tuning)
library(mlr3viz)
library(mlr3pipelines)
library(NADIA)
datos_disp <- readRDS("./datos/disp_21.rds")
datos_compet <- readRDS("./datos/compet_21.rds")
```

El \emph{dataset} `datos_disp` consta de un conjunto de datos diarios durante un periodo de 12 años (1994-2006), es decir, de un tamaño muestral $n = 12 \cdot 365 = 4380$. Además, consta de 75 variables predictoras, que consisten en 15 variables meteorológicas medidas durante 5 momentos del dia, con lo que el número de predictores es: $p = 15 \cdot 5 = 75$ y una variable respuesta, que es la radiación solar diaria de una cierta planta solar de Oklahoma.

Las variables del conjunto son las siguientes (ver siguiente página):
\clearpage
\begin{landscape}
\begin{longtable}[c]{llll}
\hline
\textbf{Variable}           & \textbf{Tipo}          & \textbf{Descripción}      &\textbf{Unidades}                                   \\ \hline
\endhead
%
\hline
\endfoot
%
\endlastfoot
%
apcp\_sfc & Numérica continua & Precipitación acumulada en la superficie durante 3 horas. & $kg/m^2$ \\
dlwrf\_sfc & Numérica continua & Promedio de flujo radiativo de onda larga en la superficie. & $W/m^2$ \\
dswrf\_sfc & Numérica continua & Promedio de flujo radiativo de onda corta en la superficie. & $W/m^2$ \\
pres\_msl & Numérica continua & Presión del aire al nivel medio del mar. & $Pa$ \\
pwat\_eatm & Numérica continua & Agua precipitable sobre toda la atmósfera. & $kg/m^2$ \\
spfh\_2m & Numérica continua & Humedad específica a 2 m sobre el suelo. & $kg/kg-l$ \\
tcdc\_eatm & Numérica continua & Cobertura total de nubes sobre toda la profundidad de la atmósfera. & $\%$ \\
tcolc\_eatm & Numérica continua & Condensado total integrado en la columna sobre toda la atmósfera. & $kg/m^2$ \\
tmax\_2m & Numérica continua & Temperatura máxima en las últimas 3 horas a 2 m sobre el suelo. & $K$ \\
tmin\_2m & Numérica continua & Temperatura mínima en las últimas 3 horas a 2 m sobre el suelo. & $K$ \\
tmp\_2m & Numérica continua & Temperatura actual a 2 m sobre el suelo. & $K$ \\
tmp\_sfc & Numérica continua & Temperatura de la superficie. & $K$ \\
ulwrf\_sfc & Numérica continua & Radiación ascendente de onda larga en la superficie. & $W/m^2$ \\
ulwrf\_tatm & Numérica continua & Radiación ascendente de onda larga en la parte superior de la atmósfera. & $W/m^2$ \\
uswrf\_sfc & Numérica continua & Radiación ascendente de onda corta en la superficie. & $W/m^2$ \\
\end{longtable}
\end{landscape}


\section{Análisis Exploratorio de Datos}
Utilizaremos el paquete `mlr3`, por lo que antes de llevar a cabo las particiones de entrenamiento y validación debemos crear una \emph{task}:
```{r}
practica_1_task <- as_task_regr(datos_disp, target = "salida", id = "radiacion")
practica_1_task$print()
practica_1_task$feature_types
practica_1_task$data()
```

Realizamos el análisis exploratorio mediante la librería `skimr`:
```{r, message = FALSE}
library(skimr)
skim_exploratorio <- skim(practica_1_task$data())
skim_exploratorio %>% filter(skim_type == "character")
skim_exploratorio %>% filter(skim_type == "factor")
skim_exploratorio %>% filter(skim_type == "numeric") %>% select(-numeric.hist, -numeric.p0, -numeric.p100)
```

Podemos ver que hay 4380 instancias y 76 atributos, de los cuáles 35 son atributos numéricos, 39 de tipo `factor` y 2 de tipo `character`. Las variables `factor` son todas completas, las `character` tienen alrededor de un 10% de NA's y en algunas numéricas como `dswrf_s1_1`, `tcolc_e4_1`, `tmp_2m_4_1` existe más de un 85% de porcentaje de NA's (las eliminaremos posteriormente durante el pre-proceso), mientras que las demás numéricas tienen un porcentaje mucho menor de datos faltantes. 
```{r, warning = FALSE, message = FALSE, fig.height = 7, fig.width=5, fig.align='center'}
(variables_numericas_muchos_NA <- skim_exploratorio %>% filter(skim_type == "numeric" & complete_rate < 0.2) %>% select(skim_variable) %>% as.data.frame() %>% as.matrix() %>% as.vector())
library(naniar)
gg_miss_var(practica_1_task$data()[], show_pct = TRUE) + ylim(1, 100)
```

Hay que diferenciar variables que toman valores constantes de aquellas que toman valores muy pequeños, ya que en ambos casos la desviación típica es igual o muy próxima a 0, estas son: `dswrf_s1_1`, `dswrf_s2_1`, `spfh_2m1_1`, `spfh_2m5_1`, `tcdc_ea5_1`, `tcolc_e1_1`,  `tcolc_e4_1`,  `tmp_sfc1_1` y `uswrf_s2_1`.
```{r}
(variables_numericas_poca_variabilidad <- skim_exploratorio %>% filter(skim_type == "numeric" & numeric.sd < 1) %>% select(skim_variable) %>% as.data.frame() %>% as.matrix() %>% as.vector())
```

Vamos a ver si realmente son constantes o solamente es que toman valores muy pequeños y por ello la desviación típica tiende a cero. Para ello nos basaremos en los histogramas:
```{r, message = FALSE}
for(i in variables_numericas_poca_variabilidad){
        datos_disp %>% select(i) %>% as.matrix() %>% as.vector() %>% na.omit() %>% hist(main = i)
}
```

Podemos ver que las variables `dswrf_s1_1`, `tcdc_ea5_1`, `tcolc_e1_1`, `tcolc_e4_1`, `tmp_sfc1_1` y `uswrf_s2_1` son efectivamente constantes, mientras que `spfh_2m1_1` y `spfh_2m5_1` tienen poca variabilidad por tener valores muy pequeños, pero sí tienen variabilidad. Con lo que guardamos dichas variables para posteriores acciones:
```{r}
variables_numericas_constantes <- c("dswrf_s1_1", "tcdc_ea5_1", "tcolc_e1_1", "tcolc_e4_1", "tmp_sfc1_1", "uswrf_s2_1")
```

Convertiremos todos los datos que no sean numéricos en `factor`, especialmente los `character`, que pueden dar problemas en futuras aplicaciones:
```{r}
variables_character <- skim_exploratorio %>% filter(skim_type == "character") %>% select(skim_variable) %>% as.matrix() %>% as.vector()
datos_disp[, variables_character[1]] <- as.factor(datos_disp[, variables_character[1]])
datos_disp[, variables_character[2]] <- as.factor(datos_disp[, variables_character[2]])
practica_1_task <- as_task_regr(datos_disp, target = "salida", id = "radiacion")
```



\subsection{Variable respuesta}
A continuación graficaremos la variable respuesta en el tiempo:
```{r}
vector_indicador <- 1:nrow(datos_disp)
datos_grafico_respuesta <- data.frame(indice = vector_indicador, salida = datos_disp$salida) 
ggplot(data = datos_grafico_respuesta, aes(x = vector_indicador, y = salida)) + geom_line() + xlab("día") + ylab("salida") + ggtitle("Radiación a lo largo del tiempo (1996-2008)")
```

Podemos ver que existe una clara estacionalidad en la radiación captada por las placas solares, aunque el nivel de la serie parece constante en el tiempo, es decir, no se observan tendencias crecientes ni decrecientes en los datos, esto es, los niveles de radiación son parecidos año tras año.

\section{Métrica: Relative Absolute Error}
El error absoluto relativo (RAE en inglés) para un modelo de regresión con variable respuesta $y_i$ puede ser definido como:
$$
RAE(\hat{y_i}) = \frac{\sum_{i=1}^n |y_i - \hat{y_i}|}{\sum_{i=1}^n |y_i - \overline{y}|}
$$
dónde $\hat{y_i}$ es la predicción de la variable respuesta que hace el modelo de regresión e $\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i$. Se puede interpretar como un ratio entre el error absoluto de predicción con el modelo escogido y el error absoluto para una predicción \emph{naive} basada en la media de la respuesta $\overline{y}$. Esta medida no está definida para el caso $y_i = y \quad \forall i=1, \dots, n$.

Este ratio puede ser usado mediante la librería `mlr3` mediante instanciado a través del diccionario `mlr_measures` o mediante la función asociada `msr()`:
```{r, message=FALSE, results=FALSE}
mlr_measures$get("regr.rae")
(measure <- msr("regr.rae"))
```


\newpage
\section{Mejor método de imputación y de escalado}
En este apartado vamos a comparar distintos métodos de imputación y escalado en base a su RAE en el conjunto de testeo para un modelo de vecino más cercano con hiper-parámetros por defecto.

\subsection{Eliminación de las variables que toman valores constantes y/o tienen muchos NA}
Eliminaremos del modelo aquellos predictores que tienen valores constante o varianzas muy próximas a cero y también aquellos que tienen un porcentaje de NA muy elevado (véase Análisis Exploratorio de Datos):
```{r}
datos_disp <- datos_disp %>% select(-(all_of(c(variables_numericas_constantes, variables_numericas_muchos_NA))))
datos_compet <- datos_compet %>% select(-(all_of(c(variables_numericas_constantes, variables_numericas_muchos_NA))))
practica_1_task <- as_task_regr(datos_disp, target = "salida", id = "radiacion")
```


\subsection{Particiones de entrenamiento y test}
A continuación dividiremos el conjunto de datos `datos_disp` en particiones de entrenamiento y testeo, correspondiendo los datos de los primeros 9 años a datos de entrenamiento, y los 3 últimos años a validación:
```{r}
set.seed(100430509) # NIA de Marc Pastor
desc_outer <- rsmp("custom")
desc_outer$instantiate(practica_1_task, 
                       train = list(1:(9*365)),
                       test = list((9*365+1):(12*365)))
```

\subsection{Métodos de escalado}
\subsubsection{Normalización de los datos}
Eliminamos las constantes, normalizamos los datos y hacemos una codificación \emph{one-hot} de las variables cualitativas:
```{r}
preproc_inicial <- po("removeconstants") %>>% po("encode")
practica_1_task <- preproc_inicial$train(practica_1_task)[[1]]

id_train <- desc_outer$train_set(i = 1)
id_test <- desc_outer$test_set(i = 1)

# Se crean dos nuevas task, una con los datos de train y otra con los de test.
# Dado que se va a aplicar un filtrado, y para no alterar task_datos, se emplea
# antes del filtro el método $clone() para hacer una copia.
task_train <- practica_1_task$clone()$filter(id_train)
task_test  <- practica_1_task$clone()$filter(id_test)
```


\subsection{Métodos de imputación multivariante}
En esta sección usaremos distintos métodos de imputación multivariante ya que disponemos de datos multivariantes, tanto variables continuas, como categóricas, etc.
\subsubsection{Imputación mediante AMELIA (\emph{Multiple Imputation of Incomplete Multivariate Data})}
AMELIA es un procedimiento para imputar datos multivariantes. Entre sus supuestos el principal es asumir que los datos (tanto observados como no) siguen una distribución normal multivariante. Si denotamos el \emph{dataset} de tamaño $(n \times k)$ como $D$, enconces esta asunción es:
$$
D \sim \mathcal{N}_k(\mu, \Sigma).
$$

En nuestro caso los datos no son solamente continuos, sino que hay variables categóricas y discretas, por lo que esta asunción no se va a dar. Por ello, descartaremos este procedimiento.

\subsubsection{MICE: \emph{Multiple Imputation by Chained Equations}}
MICE es un método de imputación múltiple que se basa en el supuesto de que dadas las variables usadas en el proceso de imputación, los datos faltantes son MAR (\emph{Missing At Random}), lo cuál significa que la probabilidad de que un valor sea faltante depende solo de los valores observados y no de los valores que no han sido observados. En otras palabras, después de controlar todos los datos disponibles (es decir, las variables incluidas en el modelo de imputación), cualquier dato faltante es completamente aleatorio. Implementar MICE cuando los datos no son MAR podría dar lugar a estimaciones sesgadas. De aquí en adelante, supondremos que nuestros datos son MAR.

Muchos de los modelos de imputación múltiples inicialmente desarrollados, asumen una distribución conjunta de todas las variables, por ejemplo la distribución normal, lo cuál no suele ocurrir en conjuntos de datos grandes, con decenas de variables de distintos tipos. MICE ofrece una alternativa flexible basada en modelos de regresión donde los datos faltantes se modelan en función de las variables disponibles en los datos. Esto implica que cada variable puede ser modelada en base a su distribución, por ejemplo las variables binarias con regresión logística y las continuas con regresión lineal, etc.

\textbf{Procedimiento MICE}

El algoritmo MICE puede ser dividido en 4 grandes pasos:
\begin{enumerate}
\item Se ejecuta una imputación simple, por ejemplo imputación mediante la media, para cada valor faltante en el \emph{dataset}. Estas imputaciones sencillas pueden ser pensadas como imputaciones base. 
\item Las imputaciones base para cada variable $X_i$ vuelven a ser asignadas el valor de faltante/missing.
\item Los valores observados de la variable $X_i$ en el paso 2 se modelan como un modelo de regresión en función del resto de variables en el \emph{dataset}. Estos modelos de regresión operan bajo los supuestos que uno haria cuando realiza regresión logística, lineal o Poisson fuera del contexto de datos faltantes.
\item Los valores faltantes de la variable $X_i$ son reemplazados por las predicciones (imputaciones) del modelo de regresión. Cuando $X_i$ posteriormente se utilice en los modelos de regresión para otras variables, se utilizarán tanto los valores observados como los imputados.
\item Se repiten los pasos 2-4 para cada variable que tiene datos faltantes. El proceso para cada una de las variables constituye una iteración o ciclo. Al final de cada ciclo, todos los valores faltantes han sido sustituidos por predicciones de regresiones que representan las relaciones entre los datos observados. 
\item Los pasos 2-4 se repiten para un número de ciclos, con las imputaciones siendo actualizadas en cada ciclo.
\end{enumerate}

El número de ciclos a realizarse puede ser escogido por el investigador, aunque generalmente se llevan a cabo 10. La idea es que al final de los ciclos, la distribución de los parámetros que gobiernan las imputaciones (por ejemplo los coeficientes de los modelos de regresión) deben haber convergido, en el sentido de volverse estables.


```{r, warning = FALSE, error = TRUE}
imp <- PipeOpMice$new()
# learner 
learner <- lrn('regr.kknn')

graph <- imp %>>% po(learner)

graph_learner <- GraphLearner$new(graph, id = 'mice.learner')
graph_learner$id <-  'mice.learner'
# resampling 
set.seed(100430509)
knn_resample <- resample(practica_1_task, graph_learner, desc_outer)
knn_rae <- knn_resample$aggregate(msr("regr.rae"))
print(knn_rae)
```
Parece que uno de los sistemas de ecuaciones es singular, por lo que el programa no encuentra solución, y por ello el algoritmo no nos es útil.

\subsubsection{missForest}
Se trata de un procedimiento que \emph{random forest} para predecir el valor de los datos faltantes:
```{r, warning = FALSE}
imp <- PipeOpmissForest$new()
# learner 
learner <- lrn('regr.kknn')

graph <- imp %>>%  learner

graph_learner <- GraphLearner$new(graph, id = 'missForest.learner')
graph_learner$id <-  'missForest.learner'
# resampling 
set.seed(100430509)
knn_resample <- resample(practica_1_task, graph_learner, desc_outer)
knn_rae <- knn_resample$aggregate(msr("regr.rae"))
print(knn_rae)
```


\subsubsection{Miss Ranger}
Utilizaremos el algoritmo `MissRanger`, que es una versión mejorada de `MissForest` en la que se añade el emparejamiento predictivo de medias entre iteraciones de los random forest. Esto evita en primer lugar imputación con valores que no estén presentes en los datos y en segundo lugar, el emparejamiento predictivo de medias intenta incrementar la varianza de las distribuciones condicionales para alcanzar un nivel realista.

```{r, warning = FALSE}
imp <- PipeOpmissRanger$new()
# learner 
learner <- lrn('regr.kknn')

graph <- imp %>>% learner

graph_learner <- GraphLearner$new(graph, id = 'missRanger.learner')
graph_learner$id <-  'missRanger.learner'
# resampling 
set.seed(100430509)
knn_resample <- resample(practica_1_task, graph_learner, desc_outer)
knn_rae <- knn_resample$aggregate(msr("regr.rae"))
print(knn_rae)
```

Ahora que hemos visto que el método \emph{MissRanger} es el que menos `rae` tiene de los que hemos probado, vamos a extraer los datos de imputados y usarlos para crear una nueva \emph{task} con ellos.

```{r}
set.seed(100430509)
graph_learner_missRanger_trained <- graph_learner$train(practica_1_task)
datos_imputados <- graph_learner_missRanger_trained$graph_model$pipeops$regr.kknn$learner_model$model
datos_imputados <- datos_imputados$data

task_DI <- practica_1_task <- as_task_regr(datos_imputados, target = "salida", id = "radiacion")
task_DI_train <- task_DI$clone()$filter(id_train)
task_DI_test  <- task_DI$clone()$filter(id_test)
```

\section{Métodos para predecir}

En esta parte del trabajo vamos a probar con varios métodos para predecir los datos de test, para luego ajustar hiperparámetros y ver si ha mejorado, empeorado, o no han habido cambios en las predicciones.

\subsection{Sin ajuste de hiperparámetros}
\subsubsection{KNN}

Primero probaremos con el método de los K vecinos más cercanos, que estima el valor de la función de probabiolidad de que un elemento $\mathcal{x}$ pertenezca a la clase $\mathcal{C_j}$ a partir de la información proporcionada por los predictores. Este es el caso de este método para clasificación, pero aquí usaremos su versión para regresión, que en lugar de establecer un sistema de votación considerando su clase, se devuelve como predicción el valor medio de es clase.

Lo primero será crear el objeto learner para este modelo.
```{r}
learner_knn <- lrn("regr.kknn")
rae_sin_hp <- function(learner, train, test, measure){
        set.seed(100430509)
        learner$train(task = train)
        predict <- learner$predict(task = test)
        rae <- predict$score(measure)
        return(list(predicciones = predict$response, RAE = rae))
}


knn_rae <- rae_sin_hp(learner_knn, task_DI_train,task_DI_test, measure)
knn_rae[2]
```     

Como podemos ver, el **Error Absouto Relativo (RAE)** para este modelo sin ajustar hiperparámetros es de `r round(knn_rae[[2]],3)`. Cuando los hayamos ajustado todos veremos cuál es el que tiene menor RAE y, por lo tanto, el mejor método para predecir sobre estos datos.

\subsubsection{Cubist}

Ahora vamos a probar con una regresión cubist, que se basa en hacer árboles de regresión que tienen modelos de regresión lineal en las hojas terminales que se basan en las predicciones hechas por los árboles que les sirven para decidir cómo se va a dividir. En este caso no haremos ajuste de hiperparámetros, por lo que voy a pasar directamente a crear el learner para este modelo y hacer las predicciones en la muestra de test.

```{r}
learner_cubist <- lrn("regr.cubist")
cubist_rae <- rae_sin_hp(learner_cubist, task_DI_train,task_DI_test, measure)
cubist_rae[2]
```

\subsubsection{rpart}

El siguiente método a probar será un arbol de regresión, que usa árboles de decisión como modelo predictivo que mapea las observaciones de los predictores. Este mapeado se hace al dividir las ramas en función de los valores que toman los predictores para separarlos en hiper-rectángulos, y la predicción de cualquier punto de un hiper-rectángulo será la media de la variable salida de las observaciones que se encuentren en ese hiper-rectángulo.

```{r}
learner_rpart <- lrn("regr.rpart")
rpart_rae <- rae_sin_hp(learner_rpart, task_DI_train,task_DI_test, measure)
rpart_rae[2]
```    
\subsubsection{RandomForest}

También vamos a ajustar un random forest, que ajusta varios árboles de regresión y crea una conmbinación de los mismos en función los valores de una distribución aleatoria que es la misma para todos los árboles.


```{r}
learner_RF <- lrn("regr.randomForest")
RF_rae <- rae_sin_hp(learner_RF, task_DI_train,task_DI_test, measure)
RF_rae[2]
```    

\subsubsection{Regresión Lineal Múltiple}

La idea principal de la regresión lineal múltiple es llenar la falta de información que provoca una distorsión en la correcta identificación de la variable y. En otras palabras, la regresión con múltiples regresores le permite medir el efecto de una variable específica $x_i$ sobre la variable $y$, manteniendo constantes las otras variables independientes. Formalmente, el modelo de regresión lineal múltiple incluye varios regresores $x_i$ y asocia a cada regresor un coeficiente $\beta_i$. El coeficiente $\beta_1$, por ejemplo, representa el cambio esperado de la variable dependiente y asociado con un cambio de unidad de $x_1$, manteniendo constantes los otros regresores.
La regresión múltiple es una extensión de la regresión lineal en relación entre más de dos variables. En la relación lineal simple tenemos un predictor y una variable de respuesta, pero en regresión múltiple tenemos más de una variable predictora y una variable de respuesta.
Los coeficientes del modelo de regresión lineal múltiple se pueden estimar utilizando el estimador clásico OLS (ordinary least squares), también conocido como el "estimador de mínimos cuadrados ordinarios". Los mínimos cuadrados ordinarios son un método muy eficaz para estimar la intersección y la pendiente de la línea de regresión. Este estimador, de hecho, determina los coeficientes beta eligiendo aquellos que minimizan la suma de los cuadrados de los errores.

Ahora vamos a ajustar el modelo.

```{r, warning=FALSE}
learner_lm <- lrn("regr.lm")
lm_rae <- rae_sin_hp(learner_lm, task_DI_train,task_DI_test, measure)
lm_rae[2]
```    


\subsubsection{SVM}

Al ser un clasificador binario supervisado, la máquina de vectores de soporte tiene el propósito de identificar un límite de separación (lineal o de otro tipo) en un espacio de elementos de tal manera que las observaciones posteriores se puedan clasificar automáticamente en grupos separados. Este límite de separación puede ser una línea unidimensional, como un plano o hiperplano, dependiendo de la dimensión en la que esté trabajando. En este caso, en el que queremos predecir la radiación solar, vamos a trabajar en un plano bidimensional.\
Son posibles muchas fronteras de decisión y la frontera de decisión debe estar tan lejos de ambas clases como sea posible, intentando de maximizar el margin. Eso da lugar a la frontera con mejor capacidad de generalización. La ubicación de los hiperplanos depende completamente de la ubicación de los vectores de soporte. Al mover un vector de soporte, el hiperplano se mueve de su ubicación original a una nueva ubicación. El desplazamiento de un vector no compatible no tiene ningún impacto en el hiperplano.\
El algoritmo SVM puede aprender límites de decisión entre clases que no son linealmente separables. Puede agregar otra dimensión a los datos, para encontrar una forma lineal de separar los datos no lineales. La dimensión extra se llama kernel (es decir, el producto escalar de los vectores en el espacio transformado), se puede imaginar como un "estiramiento" de datos en una tercera dimensión. Esta dimensión adicional le permite separar linealmente los datos. Cuando este hiperplano se proyecta sobre las dos dimensiones originales, aparece como un límite de decisión curvo. El algoritmo para encontrar el kernel utiliza una transformación matemática de los datos llamada función kernel, hay varias funciones del kernel y cada una de las cuales aplica una transformación diferente a los datos y es adecuada para encontrar límites de decisión lineales para diferentes situaciones:\
\
- Kernel lineal \
- Kernel polinómico \
- Kernel radial, gausiano \

Con el kernel lineal:

```{r}
learner_svm_lin <- lrn("regr.svm", kernel="linear", type="eps-regression")
svm_lin_rae <- rae_sin_hp(learner_svm_lin, task_DI_train,task_DI_test, measure)
svm_lin_rae[2]
```

Con el kernel radial: 

```{r}
learner_svm_rad <- lrn("regr.svm", kernel="radial", type="eps-regression")
svm_rad_rae <- rae_sin_hp(learner_svm_rad, task_DI_train,task_DI_test, measure)
svm_rad_rae[2]
```

\subsubsection{Comparación}

Una vez ajustados todos los métodos sin ajuste de hiperparámetros, vamos a ver cuál ha obtenido el menor RAE.

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c| } 
\hline
\textbf{Ajuste hiper-par}           & \textbf{KNN}          & \textbf{Cubist}      &\textbf{rpart}      &\textbf{Randomforest}      &\textbf{Regresión}      &\textbf{SVM lineal}      &\textbf{SVM radial} \\
 \hline \hline 
 Sin & `r knn_rae[2]` & `r cubist_rae[2]` & `r rpart_rae[2]` & `r RF_rae[2]` & `r lm_rae[2]` & `r svm_lin_rae[2]` & `r svm_rad_rae[2]` 
 \\
 \hline
\end{tabular}
\end{center}

Sin ajuste de hiperparámetros, el mejor modelo es el \emph{svm lineal}, con un $RAE_{svm_lin} = `r svm_lin_rae[2]`$.

\subsection{Con ajuste de hiperparámetros}

Ahora ajustaremos los hiperparámetros de algunos de estos modelos para ver si mejora su RAE y así poder elegir el mejor modelo de todos para estos datos.

\subsubsection{KNN}

Para este modelo vamos a usar un grid search a la hora de hacer el ajuste, que ajusta todos los modelos posibles dentro del espacio de búsqueda, el cual decidiremos más adelante, y elige el que mejor puntuación saque en la métrica seleccionada, en este caso el RAE.


Primero crearemos un desc_inner sacado del archivo `ResamplingHoldoutOrder.R`, ya que un custom resampling no funciona con la función `Tuner`.
```{r}
source("ResamplingHoldoutOrder.R")

desc_inner <- rsmp("holdoutorder",ratio=6/9)
```

Una vez creado este resample, vamos a definir el espacio de búsqueda para el grid search

```{r}
knn_space <- ps(k = p_int(lower=1, upper=20))
```

Con este espacio de búsqueda, el grid search ajustará todos los modelos KNN con un número de vecinos entre `r knn_space$lower` y `r knn_space$upper`. Una vez definido este espacio de búsqueda ya podemos crear el learner para el ajuste de hiperparámetros.

```{r}
rae_con_hp <- function(learner, desc_inner, measure, space, terminator, tuner, task, desc_outer){
        set.seed(100430509)
        learner_ajuste<-AutoTuner$new(
        learner = learner,
        resampling = desc_inner,
        measure = measure,
        search_space = space,
        terminator = terminator,
        tuner = tuner,
        store_tuning_instance = TRUE)
        
        #Evaluamos el learner con auto ajuste
        ajuste_resample <- resample(task,learner_ajuste,desc_outer,store_models = TRUE)
        
        # Para ver el modelo cuyos hiper-pars han sido ajustados (y sus hiper-parámetros):
        print(ajuste_resample$learners[[1]]$model$learner)
        
        return(ajuste_rae <- ajuste_resample$aggregate(measure))
        
        
}
(knn_ajuste_rae <- rae_con_hp(learner_knn, desc_inner, measure, knn_space, trm("none"), tnr("grid_search"), task_DI, desc_outer))
```

Como podemos apreciar en la salida de R de arriba, el modelo elegido ha sido el que tiene $k=20$

\subsubsection{rpart}

Para este modelo vamos a usar un random search a la hora de hacer el ajuste, que elige al azar un valor del esapcio de búsqueda y se va moviendo en un radio alrededor del mismo hasta un criterio de parada que vamos a definir con el parámetro `terminator` de la función Autotuner y elige el que mejor puntuación saque en la métrica seleccionada, en este caso el RAE.


Ya que el desc_inner es el mismo que para KNN, vamos a reutilizarlo, así que ahora crearemos el espacio de búsqueda, que en este caso será sobre dos hiperparámetros, `minsplit`, que es el mínimo de divisiones que va a hacer el árbol y `maxdepth`, que es la profundidad máxima del árbol.

```{r}
rpart_space <- ps(
  minsplit = p_int(lower = 5, upper = 20),
  maxdepth = p_int(lower = 2, upper = 30)
)
```

En este caso usaremos un espacio en el que `minsplit` vaya de `r rpart_space$lower[1]` a `r rpart_space$upper[1]` y `maxdepth` irá de `r rpart_space$lower[2]` a `r rpart_space$upper[2]`
```{r}
(rpart_ajuste_rae <- rae_con_hp(learner_rpart, desc_inner, measure, rpart_space, trm("evals", n_evals = 10), tnr("random_search"), task_DI, desc_outer))
```

En este caso, el modelo óptimo ha resultado ser el que tiene $\text{minsplit}=5$ y $\text{maxdepth}=3$, con un RAE de `r rpart_ajuste_rae`.

\subsubsection{Random forest}

En este caso ajustaremos los hiperparámetros `ntree`, que controla el número de árboles que se van a ajustar, y `maxnodes`, que es equivalente a `maxdepth`.

```{r}
RF_space <- ps(
  ntree = p_int(lower = 400, upper = 600),
  maxnodes = p_int(lower = 2, upper = 6)
)
```

En este caso usaremos un espacio en el que `ntree` vaya de `r RF_space$lower[1]` a `r RF_space$upper[1]` y `maxdepth` irá de `r RF_space$lower[2]` a `r RF_space$upper[2]`
```{r}
(RF_ajuste_rae <- rae_con_hp(learner_RF, desc_inner, measure, RF_space, trm("evals", n_evals = 10), tnr("random_search"), task_DI, desc_outer))
```

El mejor modelo que ha encontrado entre estos ha sido el que tiene $\text{ntree}=497$ y $\text{maxnodes}=6$, con un RAE de `r RF_ajuste_rae`.

\subsubsection{SVM}

Para estos modelos, lineal y radial, también usaremos random search.

Vamos con el lineal. Primero creamos es espacio de búsqueda con el hiperparámetro `cost`.

```{r}
svm_lin_space <- ps(
  cost = p_dbl(lower = 1, upper = 20),
  kernel = p_fct(levels = "linear")
)
```

En este caso usaremos un espacio en el que `cost` vaya de `r svm_lin_space$lower[1]` a `r svm_lin_space$upper[1]`
```{r}
(svm_lin_ajuste_rae <- rae_con_hp(learner_svm_lin, desc_inner, measure, svm_lin_space, trm("evals", n_evals = 10), tnr("random_search"), task_DI, desc_outer))
```
Para el radial, además de `cost`, vamos a ajustar `gamma`
```{r}
svm_rad_space <- ps(
  cost = p_dbl(lower = 1, upper = 20),
  gamma = p_dbl(lower = 1, upper = 20),
  kernel = p_fct(levels = "radial")
)
```

En este caso usaremos un espacio en el que `cost` vaya de `r svm_rad_space$lower[1]` a `r svm_rad_space$upper[1]`
```{r}
(svm_rad_ajuste_rae <- rae_con_hp(learner_svm_rad, desc_inner, measure, svm_rad_space, trm("evals", n_evals = 10), tnr("random_search"), task_DI, desc_outer))
```

\subsubsection{comparación}

Ahora vamos a añadir estos nuevos valores de RAE a la tabla anterior

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c| } 
\hline
\textbf{Ajuste hiper-par}           & \textbf{KNN}          & \textbf{Cubist}      &\textbf{rpart}      &\textbf{Randomforest}      &\textbf{Regresión}      &\textbf{SVM lineal}      &\textbf{SVM radial} \\
 \hline \hline 
 Sin & `r knn_rae[2]` & `r cubist_rae[2]` & `r rpart_rae[2]` & `r RF_rae[2]` & `r lm_rae[2]` & `r svm_lin_rae[2]` & `r svm_rad_rae[2]` \\
 \hline
 Con & `r knn_ajuste_rae` & NA & `r rpart_ajuste_rae` & `r RF_ajuste_rae` & NA & `r svm_lin_ajuste_rae` & `r svm_rad_ajuste_rae`
 \\
 \hline
\end{tabular}
\end{center}

Con ajuste de hiperparámetros, el mejor modelo es el \emph{svm lineal}, con un $RAE_{svm_lin} = `r svm_lin_ajuste_rae`$. Este valor es prácticamente el mismoq ue el de su versión sin ajuste de hiperparámetros, así que por ahorrar tiempo de computación ahora mismo nuestro modelo elegido sería el \emph{SVM con kernel lineal}.

\subsection{Métodos de ensemble}

\subsubsection{Random forest con Ranger}

Ahora vamos a usar la librería ranger para ajustar el modelo de random forest.

```{r}
library(ranger)

ranger_RF <- ranger(
formula   = salida ~ ., 
data      = datos_imputados, 
)

RAE <- function(datos,predicciones){
        rae <- sum(abs(datos - predicciones))/sum(abs(datos - mean(datos)))
        return(rae)
}

(ranger_RF_rae <- RAE(datos_imputados$salida, ranger_RF$predictions))
```

A continuación ajustaremos los hiperparámetros con esta librería.

```{r}
hyper_grid <- expand.grid(
  num_trees = seq(400, 600, by = 4),
  mtry  = seq(2, 6, by = 1),
  RAE = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger(
    formula         = salida ~ ., 
    data            = datos_imputados, 
    num.trees       = hyper_grid$num_trees[i],
    mtry =hyper_grid$node_size[i]
  )
  
  # add OOB error to grid
  hyper_grid$RAE[i] <- RAE(datos_imputados$salida, model$predictions)
}

```

\subsubsection{xgBoost e hyperband}

En este método para ajustar hiperparámetros, hyperband, los hiperparámetros son parámetros ajustables que permiten controlar el proceso de entrenamiento del modelo. El rendimiento del modelo depende en gran medida de los hiperparámetros. El ajuste de hiperparámetros es el proceso de averiguar la configuración de hiperparámetros que da como resultado un rendimiento óptimo.
La hiperbanda es un método que puede ser mucho más rápido en algunos casos que otros algoritmos de optimización bayesiana que cubren una variedad de problemas de aprendizaje profundo basados en kernel. Este método parece ser muy útil en problemas de naturaleza no estocástica. El concepto detrás de este método es asignar uniformemente un presupuesto B a un conjunto de posibles configuraciones de hiperparámetros, evaluar el rendimiento de todas las configuraciones, eliminar la peor mitad y hasta que solo quede una configuración. Dado un presupuesto B finito, los recursos B/N están distribuidos en promedio entre configuraciones, pero no está claro si se deben considerar muchas configuraciones B con un tiempo de formación promedio bajo o unas pocas B con un tiempo de entrenamiento alto.
La hiperbanda considera varios valores posibles de n dado un B fijo, esencialmente realizando una búsqueda de cuadrícula en un valor permitido de n. Cada valor de n está asociado con un recurso mínimo r que se asigna a todas las configuraciones que se eliminan y un valor mayor que n corresponde a una r menor y, por lo tanto, a una parada temprana más violenta.

Vamos a usar este sistema con un modelo xgboost, que consiste en ajustar un primer árbol, sacar los residuos y ajustar el resto de árboles a partir de los mismos.

```{r}
library(mlr3hyperband)

learner_xgboost = lrn("regr.xgboost",
  nrounds           = to_tune(p_int(27, 243, tags = "budget")),
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)

ajuste_xgboost = tune(
  method = "hyperband",
  task = task_DI,
  learner = learner_xgboost,
  resampling = desc_inner,
  measures = measure,
  eta = 3
)

learner_xgboost$param_set$values = ajuste_xgboost$result_learner_param_vals

xboost_rae <- rae_sin_hp(learner_xgboost,task_DI_train,task_DI_test,measure)
xboost_rae[2]
as.data.table(ajuste_xgboost$archive)
```

Este modelo se acerca mucho al $RAE_{svm_lin}=`rsvm_lin_rae[[2]]`$ pero, de nuevo, por esfuerzo computacional, vamos a elegir el SVM con kernel lineal

\section{Conclusión}

Como ya habíamos comentado antes, el mejor modelo ha resultado ser, tanto por RAE como por tiempo de computación, el modelo de SVM lineal sin ajuste de hiperparámetros, así que vamos a guardarlo en un archivo y a predecir los datos de competición.

```{r}
datos_compet[, variables_character[1]] <- as.factor(datos_compet[, variables_character[1]])
datos_compet[, variables_character[2]] <- as.factor(datos_compet[, variables_character[2]])
datos_compet <- datos_compet %>% select(-(all_of(c(variables_numericas_constantes, variables_numericas_muchos_NA))))
datos_compet <- cbind(rep(1,nrow(datos_compet)), datos_compet)
names(datos_compet) <- c("salida", names(datos_compet)[-1])

task_prueba <- as_task_regr(datos_compet, target = "salida", id = "compet")

preproc_inicial <- po("removeconstants") %>>% po("encode")
task_prueba <- preproc_inicial$train(task_prueba)[[1]]

imp <- PipeOpmissRanger$new()
# learner 
learner <- lrn('regr.kknn')

graph <- imp %>>% learner

graph_learner <- GraphLearner$new(graph, id = 'missRanger.learner')
graph_learner$id <-  'missRanger.learner'
# resampling 

graph_learner$train(task_prueba)
datos_imputados_compet <- graph_learner$graph_model$pipeops$regr.kknn$learner_model$model
datos_imputados_compet <- datos_imputados_compet$data

learner_svm_lin$train(task_DI_train)
save(learner_svm_lin, file = "modelo final.rds")
pred_comp <- learner_svm_lin$predict_newdata(datos_imputados_compet)
save(pred_comp$response, file = "predicciones finales.txt")
```


\newpage
\section{Bibliografía}
https://mlr3.mlr-org.com/reference/mlr_measures_regr.rae.html \\
https://gking.harvard.edu/files/gking/files/amelia_jss.pdf \\
ncbi.nlm.nih.gov/pmc/articles/PMC3074241/
